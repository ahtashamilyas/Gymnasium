{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\" style=\"font-family: 'Garamond', serif; color: #333333;\">\n",
    "    <h2 style=\"color: #8b0000; text-decoration: underline; font-variant: small-caps;\">\n",
    "        <span style=\"font-family: 'Courier New', monospace;\">&#9472;</span>\n",
    "        Reinforcement Learning Summer 2024\n",
    "        <span style=\"font-family: 'Courier New', monospace;\">&#9472;</span>\n",
    "    </h2>\n",
    "    <h2 style=\"color: #6c757d;\">Prof. Dr. Frank Kirchner</h2>\n",
    "    <h4 style=\"color: #6c757d; font-style: italic;\">Exercise Sheet â€“ I</h4>\n",
    "    <h5 style=\"color: #6c757d;\">Due: 30.04.25</h5>\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.1 (Installation & Framework)\n",
    "\n",
    "For this course, we make use of the **gymnasium framework**. Gymnasium is a standard API for reinforcement learning and also provides a broad collection of environments we will discuss during this course. The documentation of the framework can be found at [gymnasium.farama.org](http://gymnasium.farama.org).\n",
    "\n",
    "Start by installing the main framework and all the environments via the anaconda terminal with these commands:\n",
    "- pip install gynmasium\n",
    "- pip install gynmasium[all]\n",
    "\n",
    "\n",
    "### a) Try to run the code from the main page of the gymnasium documentation. \n",
    "\n",
    "You can fix possible errors related to Microsoft Visual C++ 14.0 by downloading the Microsoft C++ Build Tools and installing the missing package.\n",
    "\n",
    "**Remark**: Visualization is not possible on server-based IDEs like Google Colab.\n",
    "\n",
    "### b) Make yourself familiar with the gymnasium API, especially with the `Env` and the `Spaces` parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Install the required libraries, test them by running the code from main page & familiarise yourself with the gymnasium API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem 1.2 (Markov Decision Processes) (10 P.)\n",
    "Using the framework you are now supposed to implement a simple environment\n",
    "yourself with the help of this \n",
    "[tutorial](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/). (You do not need \n",
    "to run the code in the beginning)\n",
    "\n",
    "We have a point robot with simplified motor actions: move forward, turn 90 degrees right, and\n",
    "turn 90 degrees left. All actions can be tried in all states. A simple version of the robot world and \n",
    "its states are shown in Figure 1. The robot can assume four states for a given position shown by the arrows\n",
    "indicating the orientations of the robot (see state definition in Table 1). In the table N, E, S, and W\n",
    "stand for north, east, south, and west, respectively. If the robot is in state 0 and executes the action\n",
    "move forward, then the state of the environment does not change since the robot moves against the\n",
    "world boundary\n",
    "\n",
    "**Figure 1: The Robot world**\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"RobotWorld.jpg\" alt=\"Robot world\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  \n",
    "### Table 1: The state definition of the perceived states\n",
    "\n",
    "</p>\n",
    "\n",
    "<div style=\"margin: 0 auto; width: 50%;\">\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>State</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>4</th>\n",
    "    <th>5</th>\n",
    "    <th>6</th>\n",
    "    <th>7</th>\n",
    "    <th>8</th>\n",
    "    <th>9</th>\n",
    "    <th>10</th>\n",
    "    <th>11</th>\n",
    "    <th>12</th>\n",
    "    <th>13</th>\n",
    "    <th>14</th>\n",
    "    <th>15</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Position</strong></td>\n",
    "    <td>I</td>\n",
    "    <td>I</td>\n",
    "    <td>I</td>\n",
    "    <td>I</td>\n",
    "    <td>II</td>\n",
    "    <td>II</td>\n",
    "    <td>II</td>\n",
    "    <td>II</td>\n",
    "    <td>III</td>\n",
    "    <td>III</td>\n",
    "    <td>III</td>\n",
    "    <td>III</td>\n",
    "    <td>IV</td>\n",
    "    <td>IV</td>\n",
    "    <td>IV</td>\n",
    "    <td>IV</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Orientation</strong></td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n",
    "The task of the robot is to reach a given terminal state by executing a minimum number of actions. In this exercise, we take state 15 as a terminal state. So the robot has to reach the \n",
    "the fourth position oriented in the west direction. The dynamics of the environment are given by \n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "    P_{ss'}^a = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\text{if $s'$ is a valid next state} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{array} \\right. \\\\\n",
    "                    \\\\\n",
    "    R_{ss'}^a = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        -1 & \\text{if $s' = s$ and $s' \\neq$ terminal state}  \\\\\n",
    "        1 & \\text{if $s' \\neq s$ and $s' =$ terminal state}\\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{array} \\right.\n",
    "\\end{array}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "where $ P_{ss'}^a $ is the state transition probability and $ R_{ss'}^a $ is the expected immediate reward. One can easily see that the robot is discouraged to take actions against the world boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 A:  (5 points)** Modify the  following cell ` The robot world` according to the description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 0.8}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=4, slip_prob=0.0):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        # Add slip probability parameter (0 to 1)\n",
    "        self.slip_prob = min(1.0, max(0.0, slip_prob))  # Ensure between 0 and 1\n",
    "\n",
    "        # In this environment, we have size*4 states (size positions x 4 orientations)\n",
    "        self.observation_space = spaces.Discrete(self.size * 4)  # 4 orientations for each position\n",
    "\n",
    "        # We have 3 actions: 0=move forward, 1=turn right, 2=turn left\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Mapping of state number to position and orientation\n",
    "        self.state_to_pos_orient = {}\n",
    "        self.pos_orient_to_state = {}\n",
    "        \n",
    "        # Initialize the state mappings\n",
    "        self._init_state_mappings()\n",
    "        \n",
    "        # Orientations: 0=N, 1=E, 2=S, 3=W\n",
    "        self.orientations = [\"N\", \"E\", \"S\", \"W\"]\n",
    "        \n",
    "        # Direction vectors for each orientation [y, x]\n",
    "        self.orientation_to_direction = {\n",
    "            0: np.array([1, 0]),   # North: move up (+y)\n",
    "            1: np.array([0, 1]),   # East: move right (+x)\n",
    "            2: np.array([-1, 0]),  # South: move down (-y)\n",
    "            3: np.array([0, -1]),  # West: move left (-x)\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # The terminal state is now randomized in reset()\n",
    "        self.terminal_state = None\n",
    "\n",
    "    def _init_state_mappings(self):\n",
    "        \"\"\"Initialize mappings between state numbers and (position, orientation) pairs\"\"\"\n",
    "        state = 0\n",
    "        for position in range(self.size):  # Positions 0 to size-1\n",
    "            for orientation in range(4):  # Orientations 0-3 (N, E, S, W)\n",
    "                self.state_to_pos_orient[state] = (position, orientation)\n",
    "                self.pos_orient_to_state[(position, orientation)] = state\n",
    "                state += 1\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self._state\n",
    "\n",
    "    def _get_info(self):\n",
    "        position, orientation = self.state_to_pos_orient[self._state]\n",
    "        terminal_position, terminal_orientation = self.state_to_pos_orient[self.terminal_state]\n",
    "        \n",
    "        # Calculate Manhattan distance between current position and terminal position\n",
    "        distance = abs(position - terminal_position)\n",
    "        \n",
    "        return {\n",
    "            \"position\": position,\n",
    "            \"orientation\": self.orientations[orientation],\n",
    "            \"distance_to_goal\": distance,\n",
    "            \"is_terminal\": self._state == self.terminal_state\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose a random terminal state\n",
    "        self.terminal_state = self.np_random.integers(0, self.size * 4)\n",
    "        \n",
    "        # Start at a random state (random position and orientation)\n",
    "        self._state = self.np_random.integers(0, self.size * 4)\n",
    "        \n",
    "        # Make sure initial state is not the terminal state\n",
    "        while self._state == self.terminal_state:\n",
    "            self._state = self.np_random.integers(0, self.size * 4)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        current_position, current_orientation = self.state_to_pos_orient[self._state]\n",
    "        next_position, next_orientation = current_position, current_orientation\n",
    "        \n",
    "        # Determine if the robot slips\n",
    "        robot_slips = self.np_random.random() < self.slip_prob\n",
    "        \n",
    "        if not robot_slips:  # Only execute the action if the robot doesn't slip\n",
    "            # Process the action\n",
    "            if action == 0:  # Move forward\n",
    "                direction = self.orientation_to_direction[current_orientation]\n",
    "                next_pos_candidate = current_position + direction[1]  # Use x-component for position change\n",
    "                \n",
    "                # Check if the move is valid (not against a wall)\n",
    "                if 0 <= next_pos_candidate < self.size:\n",
    "                    next_position = next_pos_candidate\n",
    "            \n",
    "            elif action == 1:  # Turn right (90 degrees clockwise)\n",
    "                next_orientation = (current_orientation + 1) % 4\n",
    "            \n",
    "            elif action == 2:  # Turn left (90 degrees counter-clockwise)\n",
    "                next_orientation = (current_orientation - 1) % 4\n",
    "        \n",
    "        # Update the state\n",
    "        next_state = self.pos_orient_to_state[(next_position, next_orientation)]\n",
    "        self._state = next_state\n",
    "        \n",
    "        # Determine reward and termination\n",
    "        terminated = (self._state == self.terminal_state)\n",
    "        \n",
    "        # Reward logic as per the equation in the problem\n",
    "        if terminated:\n",
    "            reward = 1  # Reaching terminal state\n",
    "        elif robot_slips:\n",
    "            reward = 0  # Robot slipped, no change in state\n",
    "        elif next_position == current_position and action == 0:\n",
    "            reward = 0  # Tried to move against boundary\n",
    "        else:\n",
    "            reward = 0  # All other actions\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (self.window_size / self.size)  # The size of a single grid square in pixels\n",
    "\n",
    "        # Draw the grid\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        # Draw the current state\n",
    "        position, orientation = self.state_to_pos_orient[self._state]\n",
    "        \n",
    "        # Draw the robot as a circle\n",
    "        center_x = (position + 0.5) * pix_square_size\n",
    "        center_y = (self.size - 0.5) * pix_square_size  # Placing at the bottom row\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),  # Blue\n",
    "            (center_x, center_y),\n",
    "            pix_square_size / 4,\n",
    "        )\n",
    "        \n",
    "        # Draw the orientation indicator (arrow)\n",
    "        direction = self.orientation_to_direction[orientation]\n",
    "        arrow_length = pix_square_size / 3\n",
    "        end_x = center_x + direction[1] * arrow_length\n",
    "        end_y = center_y - direction[0] * arrow_length  # Note: y is inverted in pygame\n",
    "        pygame.draw.line(\n",
    "            canvas,\n",
    "            (255, 0, 0),  # Red\n",
    "            (center_x, center_y),\n",
    "            (end_x, end_y),\n",
    "            width=3,\n",
    "        )\n",
    "        \n",
    "        # Draw the terminal state indicator\n",
    "        term_position, term_orientation = self.state_to_pos_orient[self.terminal_state]\n",
    "        term_center_x = (term_position + 0.5) * pix_square_size\n",
    "        term_center_y = (self.size - 0.5) * pix_square_size\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 255, 0),  # Green\n",
    "            pygame.Rect(\n",
    "                term_center_x - pix_square_size/4, \n",
    "                term_center_y - pix_square_size/4,\n",
    "                pix_square_size/2, \n",
    "                pix_square_size/2\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "def run_all_tests():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 B:  (2 points)** Run the test for 1000 steps, resetting every time the robot reaches the terminal state. Save the reward after each action. Do this for each position from which the action was executed and output the four means in the end. Think about the results; do they make sense?    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Running Test 1.2 B =====\")\n",
    "env = RobotWorldEnv(render_mode=None, size=4)\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Initialize lists to store rewards for each position\n",
    "position_rewards = {0: [], 1: [], 2: [], 3: []}  # For positions I, II, III, IV\n",
    "\n",
    "# Run for 1000 steps\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Get the current position and store the reward\n",
    "    position = info[\"position\"]\n",
    "    position_rewards[position].append(reward)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Calculate and print the mean reward for each position\n",
    "print(\"Mean rewards for each position:\")\n",
    "for position in range(4):\n",
    "    position_name = [\"I\", \"II\", \"III\", \"IV\"][position]\n",
    "    mean_reward = np.mean(position_rewards[position]) if position_rewards[position] else 0\n",
    "    print(f\"Position {position_name}: {mean_reward:.4f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 C: (2 points)** Make the size of the robot world variable so that you can change it while creating the instance of `RobotWorldEnv` class. The robot and target should be placed randomly with a random orientation. Run the test as before, saving the means the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Running Test 1.2 C =====\")\n",
    "for size in [3, 5, 7]:\n",
    "    env = RobotWorldEnv(render_mode=None, size=size)\n",
    "    observation, info = env.reset(seed=42)\n",
    "    \n",
    "    # Initialize lists to store rewards for each position\n",
    "    position_rewards = {}\n",
    "    for pos in range(size):\n",
    "        position_rewards[pos] = []\n",
    "    \n",
    "    # Run for 1000 steps\n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Get the current position and store the reward\n",
    "        position = info[\"position\"]\n",
    "        position_rewards[position].append(reward)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "    \n",
    "    # Calculate and print the mean reward for each position\n",
    "    print(f\"\\nMean rewards for each position (world size {size}):\")\n",
    "    for position in range(size):\n",
    "        mean_reward = np.mean(position_rewards[position]) if position_rewards[position] else 0\n",
    "        print(f\"Position {position}: {mean_reward:.4f}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 D: (1 point)** Now, add another variable to your environment to include the probability of the robot slipping during a movement. If the agent slips, the action fails and stays in the same state as before. The slip probability variable should specify how likely it is for the robot to slip, thus should take only values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Running Test 1.2 D =====\")\n",
    "slip_probs = [0.0, 0.2, 0.5]\n",
    "results = []\n",
    "\n",
    "for slip_prob in slip_probs:\n",
    "    env = RobotWorldEnv(render_mode=None, size=4, slip_prob=slip_prob)\n",
    "    observation, info = env.reset(seed=42)\n",
    "    \n",
    "    # Initialize lists to store rewards for each position\n",
    "    position_rewards = {}\n",
    "    for pos in range(env.size):\n",
    "        position_rewards[pos] = []\n",
    "    \n",
    "    # Run for 1000 steps\n",
    "    num_steps = 0\n",
    "    num_episodes = 0\n",
    "    steps_per_episode = []\n",
    "    \n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Get the current position and store the reward\n",
    "        position = info[\"position\"]\n",
    "        position_rewards[position].append(reward)\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            steps_per_episode.append(num_steps)\n",
    "            num_steps = 0\n",
    "            num_episodes += 1\n",
    "            observation, info = env.reset()\n",
    "    \n",
    "    # Calculate and print the mean reward for each position\n",
    "    print(f\"\\nMean rewards with slip probability {slip_prob}:\")\n",
    "    mean_rewards = []\n",
    "    for position in range(env.size):\n",
    "        mean_reward = np.mean(position_rewards[position]) if position_rewards[position] else 0\n",
    "        mean_rewards.append(mean_reward)\n",
    "        print(f\"Position {position}: {mean_reward:.4f}\")\n",
    "    \n",
    "    # Calculate average steps per episode\n",
    "    avg_steps = np.mean(steps_per_episode) if steps_per_episode else 0\n",
    "    print(f\"Average steps per episode: {avg_steps:.2f}\")\n",
    "    \n",
    "    results.append((slip_prob, mean_rewards, avg_steps))\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot mean rewards by position for each slip probability\n",
    "plt.subplot(1, 2, 1)\n",
    "for slip_prob, mean_rewards, _ in results:\n",
    "    plt.plot(range(4), mean_rewards, marker='o', label=f'Slip Prob: {slip_prob}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Mean Rewards by Position')\n",
    "plt.xticks(range(4), ['I', 'II', 'III', 'IV'])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot average steps per episode for each slip probability\n",
    "plt.subplot(1, 2, 2)\n",
    "slip_probs = [r[0] for r in results]\n",
    "avg_steps = [r[2] for r in results]\n",
    "plt.plot(slip_probs, avg_steps, marker='o')\n",
    "plt.xlabel('Slip Probability')\n",
    "plt.ylabel('Average Steps per Episode')\n",
    "plt.title('Effect of Slip Probability on Episode Length')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('robot_world_results.png')\n",
    "plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem 1.3 (Dynamic Programming) (10 P.)\n",
    "\n",
    "In this problem, you will implement the policy algorithm introduced in the lecture and apply it to the toy example of a vacuum cleaner robot (see: Lecture 2). Use the provided code skeleton in the following cells to implement the algorithm. Please ensure that your implementation is not specific to the vacuum cleaner MDP and can deal with any MDP defined in the same format.\n",
    "\n",
    "**Figure 2**: The vacuum cleaner environment.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"SimpleCleaningRobot.png\" alt=\"SimpleCleaningRobot.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 A: (4 P)** Implement the policy iteration algorithm according to the given interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, mdp):\n",
    "        \"\"\"\n",
    "        Initialise all necessary variables.\n",
    "        \"\"\"\n",
    "        # TODO: IMPLEMENT ME!\n",
    "        pass\n",
    "\n",
    "    def policy_evaluation(self, policy, epsilon=1e-5):\n",
    "        \"\"\"Performs policy evaluation until max value change is under given threshold (epsilon). Returns the\n",
    "        state-value function for the given policy.\"\"\"\n",
    "        # TODO: IMPLEMENT ME!\n",
    "        pass\n",
    "\n",
    "    def policy_improvement(self, value_function):\n",
    "        \"\"\"Performs policy improvement based on the given value function. Returns the new and improved policy.\"\"\"\n",
    "        # TODO: IMPLEMENT ME!\n",
    "        pass\n",
    "\n",
    "    def run(self, max_iterations=100):\n",
    "        \"\"\"Runs the policy iteration algorithm until convergence or until the max iteration threshold is reached.\"\"\"\n",
    "        # TODO: IMPLEMENT ME!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 B: (1 P)** In the next cells, fill in the missing parts of the vacuum MDP definition in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # we define the vacuum robot example from the lecture as an MDP\n",
    "    vacuum_mdp = dict()\n",
    "    vacuum_mdp.states = np.arange(6)  # we have six states\n",
    "    vacuum_mdp.terminal_states = [0, 5]\n",
    "    vacuum_mdp.actions = np.arange(2)  # two available actions: 0 - move left, 1 - move right\n",
    "\n",
    "    # TODO: DEFINE THE TRANSITION PROBABILITIES\n",
    "    vacuum_mdp.transition_probabilities = ...\n",
    "\n",
    "    # TODO: IMPLEMENT THE REWARD FUNCTION\n",
    "    vacuum_mdp.reward_function = ...\n",
    "\n",
    "   # vacuum_pi = PolicyIteration(vacuum_mdp)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 C: (2 P)** Test your implementation and visualise the final policy and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # ...\n",
    "\n",
    "    vacuum_pi = PolicyIteration(vacuum_mdp)\n",
    "    \n",
    "    # TODO: Apply the policy iteration algorithm to the toy example\n",
    "    # and visualize the final policy and value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 D: (3 P)** Adapt your implementation to use Q-Values (state-action values) instead of state values to evaluate a given policy. Modify the class constructor to make this choice configurable by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"font-family: 'Garamond', serif; color: #333333;\">\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "    <p style=\"color: #6c757d;\">\n",
    "        Please upload your submission via StudIP by 20:00 on April 24, 2024. If you encounter any issues with the upload process, please contact me in advance at <a href=\"mailto:laux@uni-bremen.de\" style=\"color: #8b0000;\">laux@uni-bremen.de</a>. Your submission must include:\n",
    "    </p>\n",
    "    <ul style=\"list-style-type: none; color: #6c757d;\">\n",
    "        <li>&#9472; A well-documented, precompiled notebook, containing the entire solution(s) along with outputs and plots if applicable</li>\n",
    "        <li>&#9472; A markdown cell with the names and e-mail addresses of the contributing team members</li>\n",
    "    </ul>\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
