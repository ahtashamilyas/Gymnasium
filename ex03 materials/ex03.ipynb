{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"font-family: 'Garamond', serif; color: #333333;\">\n",
    "    <h2 style=\"color: #8b0000; text-decoration: underline; font-variant: small-caps;\">\n",
    "        <span style=\"font-family: 'Courier New', monospace;\">&#9472;</span>\n",
    "        Reinforcement Learning Summer 2024\n",
    "        <span style=\"font-family: 'Courier New', monospace;\">&#9472;</span>\n",
    "    </h2>\n",
    "    <h2 style=\"color: #6c757d;\">Prof. Dr. Frank Kirchner</h2>\n",
    "    <h4 style=\"color: #6c757d; font-style: italic;\">Exercise Sheet â€“ V </h4>\n",
    "    <h5 style=\"color: #6c757d;\">Due: 28.05.25</h5>\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem 3.1: Policy Gradients (10 P.)**\n",
    "\n",
    "In this exercise, you will implement a policy gradient algorithm for the Bipedal Walker environment. The goal is to familiarize yourself with the concept of policy gradients and understand its implementation using a practical example.\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"bipedal_walker.png\" alt=\"Your Image\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "\n",
    "**Policy Gradient Algorithm**\n",
    "\n",
    "INPUT: initial policy parameters $\\theta_0$  \n",
    "**for** $k=0,1,2,...$\n",
    "- Collect trajectories $D_k = {\\tau_i}$ using current policy $\\pi_k = \\pi(\\theta_k)$\n",
    "- Estimate the policy gradient as $\\hat{g}_k = \\frac{1}{|D_k|}\\sum_{\\tau \\in D}\\sum_{t=0}^{T}\\nabla_{\\theta_k} \\text{log} \\pi_{\\theta_k}\\left( a_t|s_t\\right) R(\\tau)$\n",
    "- Compute the policy update: $\\theta_{k+1} = \\theta_k + \\alpha_k \\hat{g}_k$\n",
    "\n",
    "**end for**\n",
    "\n",
    "\n",
    "</span>\n",
    "<hr>\n",
    "\n",
    "#### **3.1 A: (1 P.)** Familiarize yourself with the Bipedal Walker environment, which is available in the \"Box2D\" section of the gymnasium framework. Read the [documentation](https://gymnasium.farama.org/environments/box2d/bipedal_walker/) and understand the environment dynamics and the goal of the Bipedal Walker environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 B: (6 P.)** Implement the vanilla policy gradient algorithm for the Bipedal Walker using a neural network as the function approximator. You may use libraries such as TensorFlow or PyTorch for this implementation. You can find the pseudocode in the algorithm above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 C: (2 P.)** Train your agent in the Bipedal Walker environment for 10,000 episodes. Monitor and record the training progress, including the obtained rewards per episode. Evaluate the trained agent by running it for 10 repetitions. Measure and report the average reward achieved by the agent during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 D: (1 P.)** Repeat the training and evaluation process using different seeds until you have at least five runs. Report the mean and variance of the evaluation performance over all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Problem 3.2: Deep Q-Learning for Cart Pole (10 P.)**\n",
    "    \n",
    "In this exercise, you will implement a simple Deep Q-Learning approach for the Cart Pole problem. The goal is to familiarize yourself with the concept of Deep Q-Learning and understand its implementation using a practical example.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"cart.png\" alt=\"Your Image\" width=\"200\">\n",
    "</p>\n",
    "\n",
    "#### **3.2 A: (1 P.)** Familiarize yourself with the Cart Pole problem, which is available in the \"Classic Control\" section of the gymnasium framework. Read the [documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/) and understand the environment dynamics and the goal of the cart pole problem.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 B: (6 P.)** Implement a Deep Q-Learning algorithm for the Cart Pole problem using a neural network as the function approximator. You can use libraries such as TensorFlow or PyTorch for this implementation. The algorithm should include the following components:\n",
    "- Experience replay: Maintain a replay buffer to store and sample experiences (state, action, reward, next state, done) for training the neural network.\n",
    "- Epsilon-greedy exploration: Use an exploration strategy to balance exploration and exploitation. Initially, prioritize exploration and gradually reduce the exploration rate over time.\n",
    "- Neural network: Design and train a neural network to approximate the Q-values for different states and actions. The network should have appropriate layers and activations for the Cart Pole problem.\n",
    "- Target network: Create a separate target network to stabilize the training process. Update the target network periodically with the weights from the main network.\n",
    "- Loss function and optimization: Define an appropriate loss function, such as the mean squared error, and use an optimizer to update the neural network parameters based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 C: (2 P.)** Train your Deep Q-Learning agent on the Cart Pole problem for a sufficient number of episodes. Monitor and record the training progress, including the rewards obtained per episode and the exploration rate over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 D: (1 P.)** Evaluate the trained agent by running it on the Cart Pole problem for a fixed number of episodes. Measure and report the average reward achieved by the agent during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"font-family: 'Garamond', serif; color: #333333;\">\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "    <p style=\"color: #6c757d;\">\n",
    "        Please upload your submission via StudIP by 23:59 on May 28, 2025. If you encounter any issues with the upload process, please contact me in advance at <a href=\"mailto:laux@uni-bremen.de\" style=\"color: #8b0000;\">laux@uni-bremen.de</a>. Your submission must include:\n",
    "    </p>\n",
    "    <ul style=\"list-style-type: none; color: #6c757d;\">\n",
    "        <li>&#9472; A well-documented, precompiled notebook, containing the entire solution(s) along with outputs and plots if applicable</li>\n",
    "        <li>&#9472; A markdown cell with the names and e-mail addresses of the contributing team members</li>\n",
    "    </ul>\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
