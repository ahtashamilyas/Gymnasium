{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\" style=\"font-family: 'Garamond', serif; color: #333333;\">\n",
    "    <h2 style=\"color: #8b0000; text-decoration: underline; font-variant: small-caps;\">\n",
    "        <span style=\"font-family: 'Courier New', monospace;\">&#9472;</span>\n",
    "        Reinforcement Learning Summer 2024\n",
    "        <span style=\"font-family: 'Courier New', monospace;\">&#9472;</span>\n",
    "    </h2>\n",
    "    <h2 style=\"color: #6c757d;\">Prof. Dr. Frank Kirchner</h2>\n",
    "    <h4 style=\"color: #6c757d; font-style: italic;\">Exercise Sheet â€“ I</h4>\n",
    "    <h5 style=\"color: #6c757d;\">Due: 30.04.25</h5>\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 24px;\">\n",
    "\n",
    "- **Name:** AHTASHAM ILYAS  \n",
    "- **E-mail:** o_yfodc2@uni-bremen.de\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.1 (Installation & Framework)\n",
    "\n",
    "For this course, we make use of the **gymnasium framework**. Gymnasium is a standard API for reinforcement learning and also provides a broad collection of environments we will discuss during this course. The documentation of the framework can be found at [gymnasium.farama.org](http://gymnasium.farama.org).\n",
    "\n",
    "Start by installing the main framework and all the environments via the anaconda terminal with these commands:\n",
    "- pip install gynmasium\n",
    "- pip install gynmasium[all]\n",
    "\n",
    "\n",
    "### a) Try to run the code from the main page of the gymnasium documentation. \n",
    "\n",
    "You can fix possible errors related to Microsoft Visual C++ 14.0 by downloading the Microsoft C++ Build Tools and installing the missing package.\n",
    "\n",
    "**Remark**: Visualization is not possible on server-based IDEs like Google Colab.\n",
    "\n",
    "### b) Make yourself familiar with the gymnasium API, especially with the `Env` and the `Spaces` parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Install the required libraries, test them by running the code from main page & familiarise yourself with the gymnasium API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem 1.2 (Markov Decision Processes) (10 P.)\n",
    "Using the framework you are now supposed to implement a simple environment\n",
    "yourself with the help of this \n",
    "[tutorial](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/). (You do not need \n",
    "to run the code in the beginning)\n",
    "\n",
    "We have a point robot with simplified motor actions: move forward, turn 90 degrees right, and\n",
    "turn 90 degrees left. All actions can be tried in all states. A simple version of the robot world and \n",
    "its states are shown in Figure 1. The robot can assume four states for a given position shown by the arrows\n",
    "indicating the orientations of the robot (see state definition in Table 1). In the table N, E, S, and W\n",
    "stand for north, east, south, and west, respectively. If the robot is in state 0 and executes the action\n",
    "move forward, then the state of the environment does not change since the robot moves against the\n",
    "world boundary\n",
    "\n",
    "**Figure 1: The Robot world**\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"RobotWorld.jpg\" alt=\"Robot world\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  \n",
    "### Table 1: The state definition of the perceived states\n",
    "\n",
    "</p>\n",
    "\n",
    "<div style=\"margin: 0 auto; width: 50%;\">\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>State</th>\n",
    "    <th>0</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "    <th>4</th>\n",
    "    <th>5</th>\n",
    "    <th>6</th>\n",
    "    <th>7</th>\n",
    "    <th>8</th>\n",
    "    <th>9</th>\n",
    "    <th>10</th>\n",
    "    <th>11</th>\n",
    "    <th>12</th>\n",
    "    <th>13</th>\n",
    "    <th>14</th>\n",
    "    <th>15</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Position</strong></td>\n",
    "    <td>I</td>\n",
    "    <td>I</td>\n",
    "    <td>I</td>\n",
    "    <td>I</td>\n",
    "    <td>II</td>\n",
    "    <td>II</td>\n",
    "    <td>II</td>\n",
    "    <td>II</td>\n",
    "    <td>III</td>\n",
    "    <td>III</td>\n",
    "    <td>III</td>\n",
    "    <td>III</td>\n",
    "    <td>IV</td>\n",
    "    <td>IV</td>\n",
    "    <td>IV</td>\n",
    "    <td>IV</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Orientation</strong></td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "    <td>N</td>\n",
    "    <td>E</td>\n",
    "    <td>S</td>\n",
    "    <td>W</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n",
    "The task of the robot is to reach a given terminal state by executing a minimum number of actions. In this exercise, we take state 15 as a terminal state. So the robot has to reach the \n",
    "the fourth position oriented in the west direction. The dynamics of the environment are given by \n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "    P_{ss'}^a = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1 & \\text{if $s'$ is a valid next state} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{array} \\right. \\\\\n",
    "                    \\\\\n",
    "    R_{ss'}^a = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        -1 & \\text{if $s' = s$ and $s' \\neq$ terminal state}  \\\\\n",
    "        1 & \\text{if $s' \\neq s$ and $s' =$ terminal state}\\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{array} \\right.\n",
    "\\end{array}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "where $ P_{ss'}^a $ is the state transition probability and $ R_{ss'}^a $ is the expected immediate reward. One can easily see that the robot is discouraged to take actions against the world boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 A:  (5 points)** Modify the  following cell ` The robot world` according to the description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 0.8}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=4, slip_prob=0.0):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        \n",
    "        # Add slip probability parameter (0 to 1)\n",
    "        self.slip_prob = min(1.0, max(0.0, slip_prob))  # Ensure between 0 and 1\n",
    "\n",
    "        # In this environment, we have size*4 states (size positions x 4 orientations)\n",
    "        self.observation_space = spaces.Discrete(self.size * 4)  # 4 orientations for each position\n",
    "\n",
    "        # We have 3 actions: 0=move forward, 1=turn right, 2=turn left\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Mapping of state number to position and orientation\n",
    "        self.state_to_pos_orient = {}\n",
    "        self.pos_orient_to_state = {}\n",
    "        \n",
    "        # Initialize the state mappings\n",
    "        self._init_state_mappings()\n",
    "        \n",
    "        # Orientations: 0=N, 1=E, 2=S, 3=W\n",
    "        self.orientations = [\"N\", \"E\", \"S\", \"W\"]\n",
    "        \n",
    "        # Direction vectors for each orientation [y, x]\n",
    "        self.orientation_to_direction = {\n",
    "            0: np.array([1, 0]),   # North: move up (+y)\n",
    "            1: np.array([0, 1]),   # East: move right (+x)\n",
    "            2: np.array([-1, 0]),  # South: move down (-y)\n",
    "            3: np.array([0, -1]),  # West: move left (-x)\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # The terminal state is now randomized in reset()\n",
    "        self.terminal_state = None\n",
    "\n",
    "    def _init_state_mappings(self):\n",
    "        \"\"\"Initialize mappings between state numbers and (position, orientation) pairs\"\"\"\n",
    "        state = 0\n",
    "        for position in range(self.size):  # Positions 0 to size-1\n",
    "            for orientation in range(4):  # Orientations 0-3 (N, E, S, W)\n",
    "                self.state_to_pos_orient[state] = (position, orientation)\n",
    "                self.pos_orient_to_state[(position, orientation)] = state\n",
    "                state += 1\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self._state\n",
    "\n",
    "    def _get_info(self):\n",
    "        position, orientation = self.state_to_pos_orient[self._state]\n",
    "        terminal_position, terminal_orientation = self.state_to_pos_orient[self.terminal_state]\n",
    "        \n",
    "        # Calculate Manhattan distance between current position and terminal position\n",
    "        distance = abs(position - terminal_position)\n",
    "        \n",
    "        return {\n",
    "            \"position\": position,\n",
    "            \"orientation\": self.orientations[orientation],\n",
    "            \"distance_to_goal\": distance,\n",
    "            \"is_terminal\": self._state == self.terminal_state\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose a random terminal state\n",
    "        self.terminal_state = self.np_random.integers(0, self.size * 4)\n",
    "        \n",
    "        # Start at a random state (random position and orientation)\n",
    "        self._state = self.np_random.integers(0, self.size * 4)\n",
    "        \n",
    "        # Make sure initial state is not the terminal state\n",
    "        while self._state == self.terminal_state:\n",
    "            self._state = self.np_random.integers(0, self.size * 4)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        current_position, current_orientation = self.state_to_pos_orient[self._state]\n",
    "        next_position, next_orientation = current_position, current_orientation\n",
    "        \n",
    "        # Determine if the robot slips\n",
    "        robot_slips = self.np_random.random() < self.slip_prob\n",
    "        \n",
    "        if not robot_slips:  # Only execute the action if the robot doesn't slip\n",
    "            # Process the action\n",
    "            if action == 0:  # Move forward\n",
    "                direction = self.orientation_to_direction[current_orientation]\n",
    "                next_pos_candidate = current_position + direction[1]  # Use x-component for position change\n",
    "                \n",
    "                # Check if the move is valid (not against a wall)\n",
    "                if 0 <= next_pos_candidate < self.size:\n",
    "                    next_position = next_pos_candidate\n",
    "            \n",
    "            elif action == 1:  # Turn right (90 degrees clockwise)\n",
    "                next_orientation = (current_orientation + 1) % 4\n",
    "            \n",
    "            elif action == 2:  # Turn left (90 degrees counter-clockwise)\n",
    "                next_orientation = (current_orientation - 1) % 4\n",
    "        \n",
    "        # Update the state\n",
    "        next_state = self.pos_orient_to_state[(next_position, next_orientation)]\n",
    "        self._state = next_state\n",
    "        \n",
    "        # Determine reward and termination\n",
    "        terminated = (self._state == self.terminal_state)\n",
    "        \n",
    "        # Reward logic as per the equation in the problem\n",
    "        if terminated:\n",
    "            reward = 1  # Reaching terminal state\n",
    "        elif robot_slips:\n",
    "            reward = 0  # Robot slipped, no change in state\n",
    "        elif next_position == current_position and action == 0:\n",
    "            reward = 0  # Tried to move against boundary\n",
    "        else:\n",
    "            reward = 0  # All other actions\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (self.window_size / self.size)  # The size of a single grid square in pixels\n",
    "\n",
    "        # Draw the grid\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        # Draw the current state\n",
    "        position, orientation = self.state_to_pos_orient[self._state]\n",
    "        \n",
    "        # Draw the robot as a circle\n",
    "        center_x = (position + 0.5) * pix_square_size\n",
    "        center_y = (self.size - 0.5) * pix_square_size  # Placing at the bottom row\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),  # Blue\n",
    "            (center_x, center_y),\n",
    "            pix_square_size / 4,\n",
    "        )\n",
    "        \n",
    "        # Draw the orientation indicator (arrow)\n",
    "        direction = self.orientation_to_direction[orientation]\n",
    "        arrow_length = pix_square_size / 3\n",
    "        end_x = center_x + direction[1] * arrow_length\n",
    "        end_y = center_y - direction[0] * arrow_length  # Note: y is inverted in pygame\n",
    "        pygame.draw.line(\n",
    "            canvas,\n",
    "            (255, 0, 0),  # Red\n",
    "            (center_x, center_y),\n",
    "            (end_x, end_y),\n",
    "            width=3,\n",
    "        )\n",
    "        \n",
    "        # Draw the terminal state indicator\n",
    "        term_position, term_orientation = self.state_to_pos_orient[self.terminal_state]\n",
    "        term_center_x = (term_position + 0.5) * pix_square_size\n",
    "        term_center_y = (self.size - 0.5) * pix_square_size\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 255, 0),  # Green\n",
    "            pygame.Rect(\n",
    "                term_center_x - pix_square_size/4, \n",
    "                term_center_y - pix_square_size/4,\n",
    "                pix_square_size/2, \n",
    "                pix_square_size/2\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 B:  (2 points)** Run the test for 1000 steps, resetting every time the robot reaches the terminal state. Save the reward after each action. Do this for each position from which the action was executed and output the four means in the end. Think about the results; do they make sense?    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running Test 1.2 B =====\n",
      "Mean rewards for each position:\n",
      "Position I: 0.0365\n",
      "Position II: 0.0418\n",
      "Position III: 0.0138\n",
      "Position IV: 0.0335\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Running Test 1.2 B =====\")\n",
    "env = RobotWorldEnv(render_mode=None, size=4)\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Initialize lists to store rewards for each position\n",
    "position_rewards = {0: [], 1: [], 2: [], 3: []}  # For positions I, II, III, IV\n",
    "\n",
    "# Run for 1000 steps\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Get the current position and store the reward\n",
    "    position = info[\"position\"]\n",
    "    position_rewards[position].append(reward)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# Calculate and print the mean reward for each position\n",
    "print(\"Mean rewards for each position:\")\n",
    "for position in range(4):\n",
    "    position_name = [\"I\", \"II\", \"III\", \"IV\"][position]\n",
    "    mean_reward = np.mean(position_rewards[position]) if position_rewards[position] else 0\n",
    "    print(f\"Position {position_name}: {mean_reward:.4f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 C: (2 points)** Make the size of the robot world variable so that you can change it while creating the instance of `RobotWorldEnv` class. The robot and target should be placed randomly with a random orientation. Run the test as before, saving the means the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Test 1.2 C =====\n",
      "\n",
      "Mean rewards for each position (world size 3):\n",
      "Position 0: 0.0383\n",
      "Position 1: 0.0667\n",
      "Position 2: 0.0380\n",
      "\n",
      "Mean rewards for each position (world size 5):\n",
      "Position 0: 0.0632\n",
      "Position 1: 0.0119\n",
      "Position 2: 0.0057\n",
      "Position 3: 0.0041\n",
      "Position 4: 0.0031\n",
      "\n",
      "Mean rewards for each position (world size 7):\n",
      "Position 0: 0.0036\n",
      "Position 1: 0.0049\n",
      "Position 2: 0.0131\n",
      "Position 3: 0.0172\n",
      "Position 4: 0.0299\n",
      "Position 5: 0.0571\n",
      "Position 6: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Running Test 1.2 C =====\")\n",
    "for size in [3, 5, 7]:\n",
    "    env = RobotWorldEnv(render_mode=None, size=size)\n",
    "    observation, info = env.reset(seed=42)\n",
    "    \n",
    "    # Initialize lists to store rewards for each position\n",
    "    position_rewards = {}\n",
    "    for pos in range(size):\n",
    "        position_rewards[pos] = []\n",
    "    \n",
    "    # Run for 1000 steps\n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Get the current position and store the reward\n",
    "        position = info[\"position\"]\n",
    "        position_rewards[position].append(reward)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "    \n",
    "    # Calculate and print the mean reward for each position\n",
    "    print(f\"\\nMean rewards for each position (world size {size}):\")\n",
    "    for position in range(size):\n",
    "        mean_reward = np.mean(position_rewards[position]) if position_rewards[position] else 0\n",
    "        print(f\"Position {position}: {mean_reward:.4f}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 D: (1 point)** Now, add another variable to your environment to include the probability of the robot slipping during a movement. If the agent slips, the action fails and stays in the same state as before. The slip probability variable should specify how likely it is for the robot to slip, thus should take only values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Test 1.2 D =====\n",
      "\n",
      "Mean rewards with slip probability 0.0:\n",
      "Position 0: 0.0457\n",
      "Position 1: 0.0394\n",
      "Position 2: 0.0273\n",
      "Position 3: 0.0140\n",
      "Average steps per episode: 34.38\n",
      "\n",
      "Mean rewards with slip probability 0.2:\n",
      "Position 0: 0.0126\n",
      "Position 1: 0.0133\n",
      "Position 2: 0.0761\n",
      "Position 3: 0.0521\n",
      "Average steps per episode: 36.56\n",
      "\n",
      "Mean rewards with slip probability 0.5:\n",
      "Position 0: 0.0175\n",
      "Position 1: 0.0000\n",
      "Position 2: 0.0043\n",
      "Position 3: 0.0000\n",
      "Average steps per episode: 290.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Running Test 1.2 D =====\")\n",
    "slip_probs = [0.0, 0.2, 0.5]\n",
    "results = []\n",
    "\n",
    "for slip_prob in slip_probs:\n",
    "    env = RobotWorldEnv(render_mode=None, size=4, slip_prob=slip_prob)\n",
    "    observation, info = env.reset(seed=42)\n",
    "    \n",
    "    # Initialize lists to store rewards for each position\n",
    "    position_rewards = {}\n",
    "    for pos in range(env.size):\n",
    "        position_rewards[pos] = []\n",
    "    \n",
    "    # Run for 1000 steps\n",
    "    num_steps = 0\n",
    "    num_episodes = 0\n",
    "    steps_per_episode = []\n",
    "    \n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Get the current position and store the reward\n",
    "        position = info[\"position\"]\n",
    "        position_rewards[position].append(reward)\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            steps_per_episode.append(num_steps)\n",
    "            num_steps = 0\n",
    "            num_episodes += 1\n",
    "            observation, info = env.reset()\n",
    "    \n",
    "    # Calculate and print the mean reward for each position\n",
    "    print(f\"\\nMean rewards with slip probability {slip_prob}:\")\n",
    "    mean_rewards = []\n",
    "    for position in range(env.size):\n",
    "        mean_reward = np.mean(position_rewards[position]) if position_rewards[position] else 0\n",
    "        mean_rewards.append(mean_reward)\n",
    "        print(f\"Position {position}: {mean_reward:.4f}\")\n",
    "    \n",
    "    # Calculate average steps per episode\n",
    "    avg_steps = np.mean(steps_per_episode) if steps_per_episode else 0\n",
    "    print(f\"Average steps per episode: {avg_steps:.2f}\")\n",
    "    \n",
    "    results.append((slip_prob, mean_rewards, avg_steps))\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot mean rewards by position for each slip probability\n",
    "plt.subplot(1, 2, 1)\n",
    "for slip_prob, mean_rewards, _ in results:\n",
    "    plt.plot(range(4), mean_rewards, marker='o', label=f'Slip Prob: {slip_prob}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Mean Rewards by Position')\n",
    "plt.xticks(range(4), ['I', 'II', 'III', 'IV'])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot average steps per episode for each slip probability\n",
    "plt.subplot(1, 2, 2)\n",
    "slip_probs = [r[0] for r in results]\n",
    "avg_steps = [r[2] for r in results]\n",
    "plt.plot(slip_probs, avg_steps, marker='o')\n",
    "plt.xlabel('Slip Probability')\n",
    "plt.ylabel('Average Steps per Episode')\n",
    "plt.title('Effect of Slip Probability on Episode Length')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('robot_world_results.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem 1.3 (Dynamic Programming) (10 P.)\n",
    "\n",
    "In this problem, you will implement the policy algorithm introduced in the lecture and apply it to the toy example of a vacuum cleaner robot (see: Lecture 2). Use the provided code skeleton in the following cells to implement the algorithm. Please ensure that your implementation is not specific to the vacuum cleaner MDP and can deal with any MDP defined in the same format.\n",
    "\n",
    "**Figure 2**: The vacuum cleaner environment.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"SimpleCleaningRobot.png\" alt=\"SimpleCleaningRobot.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 A: (4 P)** Implement the policy iteration algorithm according to the given interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, mdp, use_q_values=False):\n",
    "        self.mdp = mdp\n",
    "        self.use_q_values = use_q_values\n",
    "        self.states = [int(s) for s in mdp['states']]\n",
    "        self.actions = [int(a) for a in mdp['actions']]\n",
    "        self.terminal_states = [int(s) for s in mdp['terminal_states']]\n",
    "        self.P = mdp['transition_probabilities']\n",
    "        self.R = mdp['reward_function']\n",
    "        self.policy = {s: np.random.choice(self.actions) for s in self.states if s not in self.terminal_states}\n",
    "        self.V = {s: 0 for s in self.states}\n",
    "        if use_q_values:\n",
    "            self.Q = {s: {a: 0 for a in self.actions} for s in self.states}\n",
    "\n",
    "    def policy_evaluation(self, policy, epsilon=1e-5, gamma=0.99):\n",
    "        if self.use_q_values:\n",
    "            return self._q_value_evaluation(policy, epsilon, gamma)\n",
    "        else:\n",
    "            V = self.V.copy()\n",
    "            while True:\n",
    "                delta = 0\n",
    "                for s in self.states:\n",
    "                    if s in self.terminal_states:\n",
    "                        continue\n",
    "                    a = policy[s]\n",
    "                    v = V[s]\n",
    "                    \n",
    "                    # Sum over all possible next states\n",
    "                    new_v = 0\n",
    "                    for s_ in self.P[s][a]:  # Only iterate over possible next states\n",
    "                        prob = self.P[s][a][s_]\n",
    "                        reward = self.R[s][a][s_]\n",
    "                        new_v += prob * (reward + gamma * V[int(s_)])\n",
    "                    \n",
    "                    V[s] = new_v\n",
    "                    delta = max(delta, abs(v - V[s]))\n",
    "                if delta < epsilon:\n",
    "                    break\n",
    "            return V\n",
    "\n",
    "    def _q_value_evaluation(self, policy, epsilon=1e-5, gamma=0.99):\n",
    "        Q = {s: {a: 0 for a in self.actions} for s in self.states}\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                if s in self.terminal_states:\n",
    "                    continue\n",
    "                for a in self.actions:\n",
    "                    q = Q[s][a]\n",
    "                    \n",
    "                    new_q = 0\n",
    "                    for s_ in self.P[s][a]:  # Only iterate over possible next states\n",
    "                        prob = self.P[s][a][s_]\n",
    "                        reward = self.R[s][a][s_]\n",
    "                        next_value = 0\n",
    "                        if s_ not in self.terminal_states:\n",
    "                            next_action = policy.get(int(s_), 0)  # Default to action 0 for terminal states\n",
    "                            next_value = Q[int(s_)][next_action]\n",
    "                        new_q += prob * (reward + gamma * next_value)\n",
    "                    \n",
    "                    Q[s][a] = new_q\n",
    "                    delta = max(delta, abs(q - Q[s][a]))\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        \n",
    "        # Convert Q-values to V-values\n",
    "        V = {s: max(Q[s].values()) if s not in self.terminal_states else 0 for s in self.states}\n",
    "        return V\n",
    "\n",
    "    def policy_improvement(self, V, gamma=0.99):\n",
    "        policy_stable = True\n",
    "        new_policy = self.policy.copy()\n",
    "        for s in self.states:\n",
    "            if s in self.terminal_states:\n",
    "                continue\n",
    "            old_action = self.policy[s]\n",
    "            \n",
    "            # Find the best action based on state values\n",
    "            action_values = {}\n",
    "            for a in self.actions:\n",
    "                value = 0\n",
    "                for s_ in self.P[s][a]:\n",
    "                    prob = self.P[s][a][s_]\n",
    "                    reward = self.R[s][a][s_]\n",
    "                    value += prob * (reward + gamma * V[int(s_)])\n",
    "                action_values[a] = value\n",
    "            \n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            new_policy[s] = best_action\n",
    "            if best_action != old_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        self.policy = new_policy\n",
    "        return new_policy, policy_stable\n",
    "\n",
    "    def run(self, max_iterations=100):\n",
    "        for i in range(max_iterations):\n",
    "            V = self.policy_evaluation(self.policy)\n",
    "            self.V = V\n",
    "            self.policy, stable = self.policy_improvement(V)\n",
    "            if stable:\n",
    "                print(f\"Policy converged after {i+1} iterations\")\n",
    "                break\n",
    "        return self.policy, self.V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 B: (1 P)** In the next cells, fill in the missing parts of the vacuum MDP definition in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    vacuum_mdp = dict()\n",
    "    vacuum_mdp['states'] = list(range(6))  # Use Python integers instead of np.arange\n",
    "    vacuum_mdp['terminal_states'] = [0, 5]\n",
    "    vacuum_mdp['actions'] = list(range(2))  # 0 = left, 1 = right\n",
    "\n",
    "    # Transition probabilities\n",
    "    vacuum_mdp['transition_probabilities'] = {\n",
    "        s: {\n",
    "            a: {} for a in vacuum_mdp['actions']\n",
    "        }\n",
    "        for s in vacuum_mdp['states']\n",
    "    }\n",
    "    \n",
    "    # Fill in transition probabilities\n",
    "    # State 0 is terminal - robot stays in state 0\n",
    "    vacuum_mdp['transition_probabilities'][0][0][0] = 1.0\n",
    "    vacuum_mdp['transition_probabilities'][0][1][0] = 1.0\n",
    "    \n",
    "    # State 1: move left to state 0, move right to state 2\n",
    "    vacuum_mdp['transition_probabilities'][1][0][0] = 1.0\n",
    "    vacuum_mdp['transition_probabilities'][1][1][2] = 1.0\n",
    "    \n",
    "    # State 2: move left to state 1, move right to state 3\n",
    "    vacuum_mdp['transition_probabilities'][2][0][1] = 1.0\n",
    "    vacuum_mdp['transition_probabilities'][2][1][3] = 1.0\n",
    "    \n",
    "    # State 3: move left to state 2, move right to state 4\n",
    "    vacuum_mdp['transition_probabilities'][3][0][2] = 1.0\n",
    "    vacuum_mdp['transition_probabilities'][3][1][4] = 1.0\n",
    "    \n",
    "    # State 4: move left to state 3, move right to state 5\n",
    "    vacuum_mdp['transition_probabilities'][4][0][3] = 1.0\n",
    "    vacuum_mdp['transition_probabilities'][4][1][5] = 1.0\n",
    "    \n",
    "    # State 5 is terminal - robot stays in state 5\n",
    "    vacuum_mdp['transition_probabilities'][5][0][5] = 1.0\n",
    "    vacuum_mdp['transition_probabilities'][5][1][5] = 1.0\n",
    "    \n",
    "    # Reward function - initialize with zeros\n",
    "    vacuum_mdp['reward_function'] = {\n",
    "        s: {\n",
    "            a: {s_next: 0.0 for s_next in vacuum_mdp['states']}\n",
    "            for a in vacuum_mdp['actions']\n",
    "        }\n",
    "        for s in vacuum_mdp['states']\n",
    "    }\n",
    "    \n",
    "    # Define rewards:\n",
    "    # Reward of +1 for reaching trash (state 5)\n",
    "    for s in vacuum_mdp['states']:\n",
    "        for a in vacuum_mdp['actions']:\n",
    "            vacuum_mdp['reward_function'][s][a][5] = 1.0\n",
    "    \n",
    "    # Reward of -1 for entering the power pack (state 0)\n",
    "    for s in vacuum_mdp['states']:\n",
    "        for a in vacuum_mdp['actions']:\n",
    "            vacuum_mdp['reward_function'][s][a][0] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 C: (2 P)** Test your implementation and visualise the final policy and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy converged after 3 iterations\n",
      "\n",
      "Final Policy:\n",
      "State 0 (Terminal): N/A\n",
      "State 1: Right\n",
      "State 2: Right\n",
      "State 3: Right\n",
      "State 4: Right\n",
      "State 5 (Terminal): N/A\n",
      "\n",
      "Value Function:\n",
      "State 0: 0.0000\n",
      "State 1: 0.9703\n",
      "State 2: 0.9801\n",
      "State 3: 0.9900\n",
      "State 4: 1.0000\n",
      "State 5: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # After defining the MDP\n",
    "    \n",
    "    # Initialize and run policy iteration\n",
    "    vacuum_pi = PolicyIteration(vacuum_mdp)\n",
    "    policy, value_function = vacuum_pi.run()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nFinal Policy:\")\n",
    "    policy_names = [\"Left\", \"Right\"]\n",
    "    for s in vacuum_mdp['states']:\n",
    "        if s in vacuum_mdp['terminal_states']:\n",
    "            print(f\"State {s} (Terminal): N/A\")\n",
    "        else:\n",
    "            print(f\"State {s}: {policy_names[policy[s]]}\")\n",
    "    \n",
    "    print(\"\\nValue Function:\")\n",
    "    for s, v in value_function.items():\n",
    "        print(f\"State {s}: {v:.4f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot policy (only for non-terminal states)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    states = [s for s in vacuum_mdp['states'] if s not in vacuum_mdp['terminal_states']]\n",
    "    policy_values = [policy[s] for s in states]\n",
    "    plt.bar(states, policy_values)\n",
    "    plt.xticks(vacuum_mdp['states'])\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Action (0: Left, 1: Right)\")\n",
    "    plt.title(\"Optimal Policy (non-terminal states)\")\n",
    "    \n",
    "    # Plot value function\n",
    "    plt.subplot(1, 2, 2)\n",
    "    values = [value_function[s] for s in vacuum_mdp['states']]\n",
    "    plt.bar(vacuum_mdp['states'], values)\n",
    "    plt.xticks(vacuum_mdp['states'])\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Optimal Value Function\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 D: (3 P)** Adapt your implementation to use Q-Values (state-action values) instead of state values to evaluate a given policy. Modify the class constructor to make this choice configurable by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _q_value_evaluation(self, policy, epsilon=1e-5, gamma=0.99):\n",
    "    Q = {s: {a: 0 for a in self.actions} for s in self.states}\n",
    "    V = {s: 0 for s in self.states}\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        # Update Q-values\n",
    "        for s in self.states:\n",
    "            if s in self.terminal_states:\n",
    "                continue\n",
    "                \n",
    "            for a in self.actions:\n",
    "                old_q = Q[s][a]\n",
    "                new_q = 0\n",
    "                \n",
    "                for s_next in self.P[s][a]:\n",
    "                    prob = self.P[s][a][s_next]\n",
    "                    reward = self.R[s][a][s_next]\n",
    "                    \n",
    "                    # For terminal states, value is 0\n",
    "                    if s_next in self.terminal_states:\n",
    "                        next_value = 0\n",
    "                    else:\n",
    "                        # Use the value of the next state according to the policy\n",
    "                        next_action = policy[s_next]\n",
    "                        next_value = V[s_next]\n",
    "                    \n",
    "                    new_q += prob * (reward + gamma * next_value)\n",
    "                \n",
    "                Q[s][a] = new_q\n",
    "                delta = max(delta, abs(old_q - new_q))\n",
    "        \n",
    "        # Update V based on current policy\n",
    "        for s in self.states:\n",
    "            if s in self.terminal_states:\n",
    "                V[s] = 0\n",
    "            else:\n",
    "                a = policy[s]\n",
    "                V[s] = Q[s][a]\n",
    "        \n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    self.Q = Q  # Store Q-values for later use\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running policy iteration with Q-values:\n",
      "Policy converged after 2 iterations\n",
      "\n",
      "Final Policy (Q-values):\n",
      "State 0 (Terminal): N/A\n",
      "State 1: Right\n",
      "State 2: Right\n",
      "State 3: Right\n",
      "State 4: Right\n",
      "State 5 (Terminal): N/A\n",
      "\n",
      "Value Function (Q-values):\n",
      "State 0: 0.0000\n",
      "State 1: 0.9703\n",
      "State 2: 0.9801\n",
      "State 3: 0.9900\n",
      "State 4: 1.0000\n",
      "State 5: 0.0000\n",
      "\n",
      "Q-values:\n",
      "State 1: Left: 0.0000, Right: 0.0000\n",
      "State 2: Left: 0.0000, Right: 0.0000\n",
      "State 3: Left: 0.0000, Right: 0.0000\n",
      "State 4: Left: 0.0000, Right: 0.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # After visualizing the state-value results\n",
    "    \n",
    "    # Test with Q-values\n",
    "    print(\"\\nRunning policy iteration with Q-values:\")\n",
    "    vacuum_pi_q = PolicyIteration(vacuum_mdp, use_q_values=True)\n",
    "    policy_q, value_function_q = vacuum_pi_q.run()\n",
    "    \n",
    "    print(\"\\nFinal Policy (Q-values):\")\n",
    "    for s in vacuum_mdp['states']:\n",
    "        if s in vacuum_mdp['terminal_states']:\n",
    "            print(f\"State {s} (Terminal): N/A\")\n",
    "        else:\n",
    "            print(f\"State {s}: {policy_names[policy_q[s]]}\")\n",
    "    \n",
    "    print(\"\\nValue Function (Q-values):\")\n",
    "    for s, v in value_function_q.items():\n",
    "        print(f\"State {s}: {v:.4f}\")\n",
    "    \n",
    "    # Print Q-values\n",
    "    print(\"\\nQ-values:\")\n",
    "    for s in vacuum_mdp['states']:\n",
    "        if s not in vacuum_mdp['terminal_states']:\n",
    "            left_q = vacuum_pi_q.Q[s][0]\n",
    "            right_q = vacuum_pi_q.Q[s][1]\n",
    "            print(f\"State {s}: Left: {left_q:.4f}, Right: {right_q:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"font-family: 'Garamond', serif; color: #333333;\">\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "    <p style=\"color: #6c757d;\">\n",
    "        Please upload your submission via StudIP by 20:00 on April 24, 2024. If you encounter any issues with the upload process, please contact me in advance at <a href=\"mailto:laux@uni-bremen.de\" style=\"color: #8b0000;\">laux@uni-bremen.de</a>. Your submission must include:\n",
    "    </p>\n",
    "    <ul style=\"list-style-type: none; color: #6c757d;\">\n",
    "        <li>&#9472; A well-documented, precompiled notebook, containing the entire solution(s) along with outputs and plots if applicable</li>\n",
    "        <li>&#9472; A markdown cell with the names and e-mail addresses of the contributing team members</li>\n",
    "    </ul>\n",
    "    <hr style=\"border-top: 2px solid #8b0000; width: 100%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
